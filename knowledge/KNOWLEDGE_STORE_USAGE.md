# Knowledge Store Usage Guide

## ğŸ¯ Táº¡i sao cáº§n Intelligent Pattern Classification?

TrÆ°á»›c Ä‘Ã¢y, táº¥t cáº£ patterns Ä‘á»u Ä‘Æ°á»£c lÆ°u vá»›i type `"successful_crawl"` â†’ khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c:
- Product extraction
- Article parsing  
- Review scraping
- Contact info extraction

**BÃ¢y giá»**: Patterns Ä‘Æ°á»£c classify thÃ´ng minh thÃ nh cÃ¡c loáº¡i cá»¥ thá»ƒ!

---

## ğŸ“Š Pattern Types Ä‘Æ°á»£c há»— trá»£

| Type | Khi nÃ o Ä‘Æ°á»£c classify | VÃ­ dá»¥ fields |
|------|----------------------|--------------|
| `product_list` | CÃ³ price + product name | `["product_name", "price"]` |
| `product_with_reviews` | CÃ³ product + price + rating | `["title", "price", "rating"]` |
| `product_catalog` | CÃ³ product name, khÃ´ng cÃ³ price | `["product_name", "brand"]` |
| `price_extraction` | Chá»‰ focus vÃ o giÃ¡ | `["price", "discount", "currency"]` |
| `review_extraction` | Chá»‰ focus vÃ o reviews | `["rating", "comment", "author"]` |
| `article_extraction` | CÃ³ headline/content + author/date | `["headline", "author", "published_date"]` |
| `content_extraction` | CÃ³ title/body | `["title", "content"]` |
| `contact_info` | CÃ³ email/phone/address | `["email", "phone", "address"]` |
| `navigation_pattern` | CÃ³ pagination/next_page | `["next_page", "load_more"]` |
| `tabular_data` | Nhiá»u fields (>5), numeric data | `["col1", "col2", ..., "col10"]` |
| `media_extraction` | CÃ³ image/photo URLs | `["image_url", "thumbnail"]` |
| `multi_field_extraction` | Nhiá»u fields nhÆ°ng khÃ´ng rÃµ type | 5+ fields |
| `generic_extraction` | Fallback | Báº¥t ká»³ |

---

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Retrieve patterns theo type

```python
# TÃ¬m táº¥t cáº£ product extraction patterns Ä‘Ã£ há»c
product_patterns = await knowledge_store.get_patterns_by_type(
    pattern_type="product_list",
    top_k=10
)

for pattern in product_patterns:
    print(f"Domain: {pattern['domain']}")
    print(f"Success rate: {pattern['success_rate']}")
    print(f"Fields: {pattern['extraction_fields']}")
```

### 2. Láº¥y best practices

```python
# Best practices cho amazon.com
best = await knowledge_store.get_best_practices(
    domain="amazon.com",
    pattern_type="product_with_reviews"
)

# Best practices cho táº¥t cáº£ article extraction
article_best = await knowledge_store.get_best_practices(
    pattern_type="article_extraction"
)
```

### 3. Semantic search vá»›i enriched query

```python
# Query vá»›i context Ä‘áº§y Ä‘á»§
patterns = await knowledge_store.retrieve_patterns(
    query={
        "domain": "shopee.vn",
        "intent": "extract product information",
        "description": "Get product name, price, and ratings from listing page",
        "extraction_fields": ["product_name", "price", "rating"],
        "user_description": "Crawl product prices with reviews",
        "include_related": True  # Graph enrichment
    },
    top_k=5
)

# Káº¿t quáº£:
# - Top matches tá»« Qdrant (semantic similarity)
# - Related patterns tá»« Neo4j (cÃ¹ng domain, similar patterns)
```

### 4. Pattern statistics

```python
stats = await knowledge_store.get_pattern_statistics()

print(f"Total patterns: {stats['total_patterns']}")
print(f"\nBy type:")
for pattern_type, count in stats['by_type'].items():
    print(f"  {pattern_type}: {count}")

print(f"\nBy domain:")
for domain, count in stats['by_domain'].items():
    print(f"  {domain}: {count}")

print(f"\nHigh success patterns (>80%): {stats['high_success_patterns']}")
print(f"Frequently used (>5 times): {stats['frequently_used']}")

# Pagination statistics
pagination_stats = stats['pagination_stats']
print(f"\nğŸ”„ Pagination:")
print(f"  Patterns with pagination: {pagination_stats['patterns_with_pagination']}")
print(f"  Average pages crawled: {pagination_stats['avg_pages_crawled']:.1f}")
print(f"  Strategies used:")
for strategy, count in pagination_stats['strategies_used'].items():
    print(f"    {strategy}: {count}")
```

### 5. Retrieve pagination patterns

```python
# Get successful pagination patterns for a domain
pagination_patterns = await knowledge_store.get_pagination_patterns(
    domain="shopee.vn",
    pattern_type="product_list",
    top_k=5
)

for pattern in pagination_patterns:
    pagination_info = pattern['pagination_info']
    print(f"Domain: {pattern['domain']}")
    print(f"  Strategy: {pagination_info['pagination_strategy']}")
    print(f"  Pages crawled: {pagination_info['pages_crawled']}")
    print(f"  Max requested: {pagination_info['max_pages_requested']}")
    print(f"  Success rate: {pattern['success_rate']:.2f}")
```

---

## ğŸ” Flow hoÃ n chá»‰nh

### Training Phase:

```python
# 1. User crawl amazon.com for products
task = {
    "url": "https://amazon.com/products",
    "user_description": "Extract product names and prices",
    "extraction_schema": {"required": ["product_name", "price"]}
}

# 2. Agent executes â†’ result cÃ³ data
result = {
    "success": True,
    "data": [
        {"product_name": "iPhone 15", "price": "$999"},
        {"product_name": "MacBook Pro", "price": "$2499"}
    ]
}

# 3. Algorithm learns â†’ classify thÃ´ng minh
rollout_data = [{
    "reward": 0.95,
    "task": task,
    "result": result
}]

learned = await algorithm.learn_from_interactive_rollouts(rollout_data)

# Pattern Ä‘Æ°á»£c lÆ°u vá»›i:
# - type: "product_list" (intelligent!)
# - extraction_fields: ["product_name", "price"]
# - domain: "amazon.com"
# - success_rate: 0.95
```

### Retrieval Phase:

```python
# User má»›i muá»‘n crawl ebay.com (domain khÃ¡c nhÆ°ng intent giá»‘ng)
new_task = {
    "url": "https://ebay.com/items",
    "user_description": "Get product titles and prices"
}

# Semantic search tÃ¬m patterns tÆ°Æ¡ng tá»±
similar_patterns = await knowledge_store.retrieve_patterns({
    "domain": "ebay.com",
    "intent": "product extraction",
    "description": new_task["user_description"],
    "extraction_fields": ["product_title", "price"]  # Infer from description
})

# Káº¿t quáº£:
# 1. Pattern tá»« amazon.com (type: product_list, score: 0.92)
#    â†’ VÃ¬ semantic giá»‘ng: "product + price"
# 2. Pattern tá»« shopee.vn (type: product_with_reviews, score: 0.85)
#    â†’ Graph enrichment: cÃ¹ng category
```

---

## ğŸ’¡ Lá»£i Ã­ch

### Before (hardcoded `"successful_crawl"`):
```json
{
  "type": "successful_crawl",
  "domain": "amazon.com",
  "description": "Successful crawl for https://amazon.com"
}
```
âŒ KhÃ´ng biáº¿t pattern nÃ y extract cÃ¡i gÃ¬  
âŒ Semantic search khÃ´ng hiá»‡u quáº£  
âŒ KhÃ´ng group Ä‘Æ°á»£c theo loáº¡i  

### After (intelligent classification):
```json
{
  "type": "product_with_reviews",
  "domain": "amazon.com",
  "extraction_fields": ["product_name", "price", "rating", "review_count"],
  "description": "product_with_reviews pattern for amazon.com",
  "metadata": {
    "user_prompt": "Get products with ratings",
    "items_extracted": 50,
    "pagination": {
      "used_pagination": true,
      "pages_crawled": 5,
      "pagination_strategy": "click_next_button",
      "max_pages_requested": 10,
      "pagination_successful": true
    }
  }
}
```
âœ… Biáº¿t rÃµ pattern nÃ y lÃ m gÃ¬  
âœ… Semantic search chÃ­nh xÃ¡c  
âœ… Group Ä‘Æ°á»£c theo type  
âœ… Best practices per type  
âœ… **Biáº¿t pattern nÃ y dÃ¹ng pagination nhÆ° tháº¿ nÃ o**  

---

## ğŸ¯ Use cases thá»±c táº¿

### 1. Type-specific strategy selection

```python
# Agent quyáº¿t Ä‘á»‹nh strategy dá»±a trÃªn pattern type
user_wants = "extract product prices"

# TÃ¬m best product_list patterns
strategies = await knowledge_store.get_best_practices(
    pattern_type="product_list"
)

# Apply strategy tá»« pattern cÃ³ success_rate cao nháº¥t
best_strategy = strategies[0]
agent.apply_strategy(best_strategy)
```

### 2. Domain adaptation

```python
# User chÆ°a tá»«ng crawl target.com nhÆ°ng Ä‘Ã£ cÃ³ walmart.com
target_patterns = await knowledge_store.retrieve_patterns({
    "domain": "target.com",
    "intent": "product extraction",
    "extraction_fields": ["name", "price"]
})

# Graph enrichment sáº½ tráº£ vá»:
# - Walmart patterns (cÃ¹ng industry: retail)
# - Amazon patterns (cÃ¹ng type: product_list)
# â†’ Agent cÃ³ baseline strategy ngay láº­p tá»©c!
```

### 3. Failure analysis

```python
# TÃ¬m xem pattern type nÃ o hay fail
stats = await knowledge_store.get_pattern_statistics()

failure_patterns = await knowledge_store.get_patterns_by_type(
    pattern_type="failure_pattern",
    top_k=20
)

# PhÃ¢n tÃ­ch: "article_extraction hay fail vÃ¬ selector thay Ä‘á»•i"
# â†’ Cáº£i thiá»‡n strategy cho type Ä‘Ã³
```

### 4. Learn pagination strategies

```python
# User muá»‘n crawl e-commerce site vá»›i nhiá»u pages
# TÃ¬m patterns Ä‘Ã£ thÃ nh cÃ´ng vá»›i pagination

pagination_patterns = await knowledge_store.get_pagination_patterns(
    pattern_type="product_list",
    top_k=5
)

# Analyze successful strategies
for pattern in pagination_patterns:
    pagination = pattern['pagination_info']
    print(f"Domain: {pattern['domain']}")
    print(f"  Strategy: {pagination['pagination_strategy']}")
    print(f"  Success: {pattern['success_rate']:.2f}")
    print(f"  Pages: {pagination['pages_crawled']}")

# Apply best strategy
best_pagination = pagination_patterns[0]['pagination_info']
agent.set_pagination_strategy(best_pagination['pagination_strategy'])
```

---

## ğŸ”§ TÃ­ch há»£p vÃ o Agent

```python
class SharedCrawlerAgent(_BaseAgent):
    def __init__(self, gemini_client, mode: str = "production", 
                 knowledge_store: Optional[HybridKnowledgeStore] = None):
        self.knowledge_store = knowledge_store
        # ...
    
    async def _training_rollout(self, task, resources, rollout):
        # BÆ°á»›c 1: Láº¥y base resources (versioned)
        learned_patterns = resources.get("domain_patterns", {})
        
        # BÆ°á»›c 2: Enrich vá»›i real-time semantic search
        if self.knowledge_store:
            # Infer extraction fields from task
            extraction_fields = []
            if task.get("extraction_schema"):
                extraction_fields = task["extraction_schema"].get("required", [])
            
            query = {
                "domain": self._extract_domain(task["url"]),
                "intent": task.get("user_description", ""),
                "description": task.get("user_description", ""),
                "extraction_fields": extraction_fields,
                "include_related": True
            }
            
            # Semantic search + graph enrichment
            similar_patterns = await self.knowledge_store.retrieve_patterns(
                query, top_k=3
            )
            
            # Apply best matching pattern
            if similar_patterns and similar_patterns[0].get("score", 0) > 0.85:
                best_pattern = similar_patterns[0]
                logger.info(f"ğŸ¯ Using learned pattern: {best_pattern['type']} "
                          f"(score: {best_pattern['score']:.2f})")
                # Merge strategy...
```

---

## ğŸ“ˆ Monitoring

```python
# Xem knowledge store Ä‘ang há»c gÃ¬
async def monitor_knowledge():
    stats = await knowledge_store.get_pattern_statistics()
    
    print(f"ğŸ“Š KNOWLEDGE STORE STATUS")
    print(f"=" * 60)
    print(f"Total patterns: {stats['total_patterns']}")
    print(f"High-success patterns: {stats['high_success_patterns']}")
    print(f"Frequently used: {stats['frequently_used']}")
    print(f"\nPattern types:")
    for ptype, count in sorted(stats['by_type'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {ptype:30s} {count:3d}")
    print(f"\nTop domains:")
    for domain, count in sorted(stats['by_domain'].items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {domain:30s} {count:3d}")
```

Output:
```
ğŸ“Š KNOWLEDGE STORE STATUS
============================================================
Total patterns: 245
High-success patterns: 178
Frequently used: 45

Pattern types:
  product_list                   89
  article_extraction             45
  product_with_reviews           32
  review_extraction              28
  price_extraction               18
  contact_info                   12
  ...

Top domains:
  amazon.com                     45
  shopee.vn                      38
  ebay.com                       27
  ...

ğŸ”„ Pagination:
  Patterns with pagination: 67
  Average pages crawled: 4.2
  Strategies used:
    click_next_button: 45
    url_navigation: 18
    infinite_scroll: 4
```

---

## âœ… Checklist Implementation

- [x] Intelligent pattern classification trong `learn_from_interactive_rollouts()`
- [x] LÆ°u `extraction_fields` vÃ o pattern
- [x] Enriched embedding (type + domain + fields + user_prompt)
- [x] `get_patterns_by_type()` - filter theo type
- [x] `get_best_practices()` - high success + high frequency
- [x] `get_pattern_statistics()` - overview
- [ ] TÃ­ch há»£p vÃ o `SharedCrawlerAgent` (TODO)
- [ ] Real-time semantic search trong agent runtime (TODO)

---

## ğŸ“ Káº¿t luáº­n

**TrÆ°á»›c**: Knowledge store = "black box" chá»©a "successful_crawl"  
**Sau**: Knowledge store = "smart library" vá»›i classified patterns, semantic search, vÃ  best practices

BÃ¢y giá» agent cÃ³ thá»ƒ:
1. Há»c tá»« patterns cá»¥ thá»ƒ (product, article, review, etc.)
2. TÃ¬m kiáº¿m semantic chÃ­nh xÃ¡c
3. Ãp dá»¥ng best practices theo type
4. Adapt nhanh cho domains má»›i
